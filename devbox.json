{
  "$schema": "https://raw.githubusercontent.com/jetpack-io/devbox/main/internal/devpkg/devbox.schema.json",
  "packages": [
    "nodejs@22",
    "python@3.12",
    "python312Packages.pip",
    "git",
    "gh",
    "jq",
    "yq",
    "ripgrep",
    "fd",
    "bat",
    "fzf",
    "tree",
    "curl",
    "httpie",
    "typescript",
    "postgresql@16",
    "stripe-cli",
    "supabase-cli"
  ],
  "shell": {
    "init_hook": [
      "# Cross-platform compatibility for tools",
      "if command -v fdfind >/dev/null 2>&1 && ! command -v fd >/dev/null 2>&1; then alias fd=fdfind; fi",
      "if command -v batcat >/dev/null 2>&1 && ! command -v bat >/dev/null 2>&1; then alias bat=batcat; fi",
      "",
      "# Essential aliases",
      "alias ll='ls -alF'",
      "",
      "# Node.js memory settings",
      "export NODE_OPTIONS='--max-old-space-size=4096'",
      "",
      "# Set npm prefix to local directory to avoid permission issues",
      "export NPM_CONFIG_PREFIX=\"$HOME/.npm-global\"",
      "export PATH=\"$HOME/.npm-global/bin:$PATH\"",
      "mkdir -p \"$HOME/.npm-global\"",
      "",
      "# Python virtual environment setup",
      "if [ ! -d venv ]; then",
      "  echo 'Creating Python virtual environment...'",
      "  python3 -m venv venv",
      "fi",
      "source venv/bin/activate",
      "",
      "# Ensure critical vLLM dependencies are always checked and installed",
      "echo 'Checking vLLM dependencies...'",
      "MISSING_DEPS=false",
      "",
      "# Check each critical dependency",
      "for pkg in torch vllm transformers accelerate modelscope pillow boto3 FlagEmbedding supabase; do",
      "  # Handle special case: pillow imports as PIL",
      "  import_name=$pkg",
      "  if [ \"$pkg\" = \"pillow\" ]; then",
      "    import_name=\"PIL\"",
      "  fi",
      "  if ! python3 -c \"import $import_name\" 2>/dev/null; then",
      "    echo \"  Missing: $pkg\"",
      "    MISSING_DEPS=true",
      "  fi",
      "done",
      "",
      "# Install missing dependencies",
      "if [ \"$MISSING_DEPS\" = true ]; then",
      "  echo 'Installing missing dependencies...'",
      "  pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121 2>/dev/null",
      "  pip install -q vllm transformers accelerate 'modelscope>=1.18.1' pillow boto3 FlagEmbedding supabase 2>/dev/null",
      "fi",
      "",
      "# Note: FlashAttention is not compatible with PyTorch 2.7+ due to ABI changes",
      "# vLLM will automatically use xFormers backend which provides good performance",
      "",
      "# Configure CUDA for PyTorch (if available)",
      "if [ -d /usr/local/cuda-12.6 ]; then",
      "  export CUDA_HOME=/usr/local/cuda-12.6",
      "  export PATH=/usr/local/cuda-12.6/bin:$PATH",
      "  # Add CUDA libs and WSL NVIDIA driver libs",
      "  export LD_LIBRARY_PATH=/usr/lib/wsl/lib:/usr/local/cuda-12.6/lib64${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}",
      "fi",
      "",
      "# Configure pip to handle SSL issues",
      "export PIP_TRUSTED_HOST=\"pypi.org pypi.python.org files.pythonhosted.org\"",
      "export PIP_CERT=\"\"",
      "",
      "# Install Python packages if requirements.txt exists",
      "if [ -f requirements.txt ]; then",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org -q -r requirements.txt 2>/dev/null || echo 'Note: Some pip packages may have failed to install'",
      "elif [ ! -f venv/.packages_installed ]; then",
      "  echo 'Installing Python packages for image processing...'",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --upgrade pip 2>/dev/null || true",
      "  # Install PyTorch with CUDA 12.1 (compatible with system CUDA 12.0)",
      "  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 2>/dev/null || echo 'PyTorch installation may need manual intervention'",
      "  # Install vLLM and required packages",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org vllm transformers accelerate 'modelscope>=1.18.1' 2>/dev/null || echo 'Note: Some packages may not have installed'",
      "  # Install other utility packages",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org pillow requests python-dotenv boto3 2>/dev/null || echo 'Note: Some packages may not have installed'",
      "  touch venv/.packages_installed",
      "fi",
      "",
      "# Install npm tools locally if not present",
      "if ! command -v wrangler >/dev/null 2>&1; then",
      "  echo 'Installing wrangler locally...'",
      "  npm install --prefix \"$HOME/.npm-global\" -g wrangler 2>/dev/null || echo 'wrangler may need to be installed separately'",
      "fi",
      "",
      "if ! command -v zuplo >/dev/null 2>&1; then",
      "  echo 'Installing zuplo CLI locally...'",
      "  npm install --prefix \"$HOME/.npm-global\" -g @zuplo/cli 2>/dev/null || echo 'zuplo CLI may need to be installed separately'",
      "fi",
      "",
      "if ! command -v playwright >/dev/null 2>&1; then",
      "  echo 'Installing playwright locally...'",
      "  npm install --prefix \"$HOME/.npm-global\" -g @playwright/test 2>/dev/null || echo 'playwright may need to be installed separately'",
      "fi",
      "",
      "# Auto-detect GPU and configure LLM backend",
      "echo 'Detecting hardware configuration for LLM inference...'",
      "# Find the project root - prefer current directory if devbox.json exists here",
      "if [ -f \"./devbox.json\" ]; then",
      "  PROJECT_ROOT=\".\"",
      "else",
      "  PROJECT_ROOT=\"$(dirname \"$(find . -maxdepth 3 -name devbox.json -type f 2>/dev/null | head -1)\" 2>/dev/null)\"",
      "  if [ -z \"$PROJECT_ROOT\" ]; then PROJECT_ROOT=\".\"; fi",
      "fi",
      "if [ -f \"$PROJECT_ROOT/backend/scripts/detect-gpu-config.sh\" ]; then",
      "  bash \"$PROJECT_ROOT/backend/scripts/detect-gpu-config.sh\" > /dev/null 2>&1",
      "  if [ -f \"$PROJECT_ROOT/backend/scripts/auto-config.json\" ]; then",
      "    # Parse the configuration",
      "    CONFIG_FILE=\"$PROJECT_ROOT/backend/scripts/auto-config.json\"",
      "    BACKEND=$(python3 -c \"import json; config=json.load(open('$CONFIG_FILE')); print(config.get('backend', 'unknown'))\" 2>/dev/null || echo 'unknown')",
      "    MODEL=$(python3 -c \"import json; config=json.load(open('$CONFIG_FILE')); print(config.get('model', 'unknown'))\" 2>/dev/null || echo 'unknown')",
      "    QUANTIZATION=$(python3 -c \"import json; config=json.load(open('$CONFIG_FILE')); print(config.get('quantization', 'None'))\" 2>/dev/null || echo 'None')",
      "    echo \"  ‚úì Hardware detected: Backend=$BACKEND, Model=$(echo $MODEL | rev | cut -d'/' -f1 | rev)\"",
      "    if [ \"$QUANTIZATION\" != \"None\" ] && [ \"$QUANTIZATION\" != \"null\" ]; then",
      "      echo \"    Quantization: $QUANTIZATION\"",
      "    fi",
      "  else",
      "    echo '  ‚ö† Hardware detection completed but no configuration generated'",
      "  fi",
      "else",
      "  echo '  ‚Ñπ GPU detection script not found. Run from project root to enable auto-configuration.'",
      "fi",
      "",
      "echo ''",
      "echo 'üöÄ iCraftStories Development Environment Ready'",
      "echo \"Node version: $(node -v)\"",
      "echo \"Python version: $(python3 --version)\"",
      "echo 'Virtual env: venv (activated)'",
      "echo 'NPM global packages: $HOME/.npm-global'",
      "",
      "# Show LLM server startup command if configured",
      "if [ -f \"$PROJECT_ROOT/backend/scripts/auto-config.json\" ]; then",
      "  echo ''",
      "  echo 'üí° To start the LLM server with auto-detected configuration:'",
      "  echo '   python3 backend/scripts/auto-start-llm.py'",
      "  echo ''",
      "  echo '   Or use specific servers:'",
      "  if [ \"$BACKEND\" = \"vllm\" ] || [ \"$BACKEND\" = \"vllm-awq\" ]; then",
      "    echo '   python3 backend/scripts/run-vllm-server.py  # vLLM with Qwen2.5-VL'",
      "  elif [ \"$BACKEND\" = \"llama.cpp\" ]; then",
      "    echo '   ./llama-server -m models/qwen2.5-vl-7b-q4_k_m.gguf  # llama.cpp'",
      "  elif [ \"$BACKEND\" = \"mlx\" ]; then",
      "    echo '   python3 -m mlx_lm.server --model mlx-community/Qwen2.5-VL-7B-4bit  # MLX'",
      "  fi",
      "fi",
      "",
      "# Show image processing pipeline commands",
      "echo ''",
      "echo 'üñºÔ∏è  Image Processing Pipeline:'",
      "echo '   cd backend/scripts'",
      "echo '   ./process-pipeline.sh status      # Check pipeline status'",
      "echo '   ./process-pipeline.sh full        # Run complete pipeline'",
      "echo '   ./process-pipeline.sh categorize  # Process images with Qwen2.5-VL'",
      "echo '   ./process-pipeline.sh embeddings  # Generate BGE-M3 embeddings'",
      "echo '   ./process-pipeline.sh deploy      # Deploy to Supabase databases'",
      "echo ''",
      "echo 'üìö For more info: cat backend/scripts/README_IMAGE_PIPELINE.md'"
    ]
  },
  "env": {
    "NODE_ENV": "development",
    "FORCE_COLOR": "1"
  }
}