{
  "$schema": "https://raw.githubusercontent.com/jetpack-io/devbox/main/internal/devpkg/devbox.schema.json",
  "packages": [
    "nodejs@22",
    "python@3.12",
    "python312Packages.pip@latest",
    "git@latest",
    "gh@latest",
    "jq@latest",
    "yq@latest",
    "ripgrep@latest",
    "fd@latest",
    "bat@latest",
    "fzf@latest",
    "tree@latest",
    "curl@latest",
    "httpie@latest",
    "typescript@latest",
    "postgresql@16",
    "stripe-cli@latest",
    "supabase-cli@latest",
    "libwebp@latest",
    "librsvg@latest",
    "pkg-config@latest",
    "cairo@latest",
    "imagemagick@latest",
    "playwright@latest"
  ],
  "shell": {
    "init_hook": [
      "# Cross-platform compatibility for tools",
      "if command -v fdfind >/dev/null 2>&1 && ! command -v fd >/dev/null 2>&1; then alias fd=fdfind; fi",
      "if command -v batcat >/dev/null 2>&1 && ! command -v bat >/dev/null 2>&1; then alias bat=batcat; fi",
      "",
      "# Essential aliases",
      "alias ll='ls -alF'",
      "",
      "# Node.js memory settings",
      "export NODE_OPTIONS='--max-old-space-size=4096'",
      "",
      "# Set npm prefix to local directory to avoid permission issues",
      "export NPM_CONFIG_PREFIX=\"$HOME/.npm-global\"",
      "export PATH=\"$HOME/.npm-global/bin:$PATH\"",
      "mkdir -p \"$HOME/.npm-global\"",
      "",
      "# Python virtual environment setup",
      "if [ ! -d venv ]; then",
      "  echo 'Creating Python virtual environment...'",
      "  python3 -m venv venv",
      "fi",
      "source venv/bin/activate",
      "",
      "# Ensure critical vLLM dependencies are installed",
      "echo 'Checking vLLM dependencies...'",
      "MISSING_DEPS=false",
      "MISSING_LIST=\"\"",
      "",
      "# Check each critical dependency",
      "for pkg in requests torch vllm transformers accelerate modelscope pillow boto3 FlagEmbedding supabase cairosvg psutil; do",
      "  # Handle special case: pillow imports as PIL",
      "  import_name=$pkg",
      "  if [ \"$pkg\" = \"pillow\" ]; then",
      "    import_name=\"PIL\"",
      "  fi",
      "  # Check if package can be imported",
      "  if python3 -c \"import $import_name\" >/dev/null 2>&1; then",
      "    : # Package found, do nothing",
      "  else",
      "    echo \"  Missing: $pkg\"",
      "    MISSING_LIST=\"$MISSING_LIST $pkg\"",
      "    MISSING_DEPS=true",
      "  fi",
      "done",
      "",
      "# Install missing dependencies",
      "if [ \"$MISSING_DEPS\" = true ]; then",
      "  echo \"Installing missing dependencies:$MISSING_LIST\"",
      "  # Install torch first if needed",
      "  if echo \"$MISSING_LIST\" | grep -q \"torch\"; then",
      "    pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121 2>/dev/null",
      "  fi",
      "  # Install all other missing packages",
      "  if [ -n \"$MISSING_LIST\" ]; then",
      "    pip install -q $MISSING_LIST 2>/dev/null",
      "  fi",
      "else",
      "  echo '  ✓ All dependencies installed'",
      "fi",
      "",
      "# Note: FlashAttention is not compatible with PyTorch 2.7+ due to ABI changes",
      "# vLLM will automatically use xFormers backend which provides good performance",
      "",
      "# Configure Cairo library path for cairosvg",
      "# Find Cairo library in Nix store",
      "CAIRO_LIB_PATHS=$(find /nix/store -name 'libcairo.so.2' 2>/dev/null | head -1 | xargs dirname 2>/dev/null)",
      "if [ -n \"$CAIRO_LIB_PATHS\" ]; then",
      "  export LD_LIBRARY_PATH=\"$CAIRO_LIB_PATHS${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}\"",
      "  echo \"  ✓ Cairo library configured for SVG support\"",
      "else",
      "  # Fallback: try pkg-config",
      "  if command -v pkg-config >/dev/null 2>&1; then",
      "    CAIRO_LIB_PATH=$(pkg-config --variable=libdir cairo 2>/dev/null)",
      "    if [ -n \"$CAIRO_LIB_PATH\" ]; then",
      "      export LD_LIBRARY_PATH=\"$CAIRO_LIB_PATH${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}\"",
      "    fi",
      "  fi",
      "fi",
      "",
      "# Configure CUDA for PyTorch (if available)",
      "if [ -d /usr/local/cuda-12.6 ]; then",
      "  export CUDA_HOME=/usr/local/cuda-12.6",
      "  export PATH=/usr/local/cuda-12.6/bin:$PATH",
      "  # Add CUDA libs and WSL NVIDIA driver libs",
      "  export LD_LIBRARY_PATH=/usr/lib/wsl/lib:/usr/local/cuda-12.6/lib64${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}",
      "fi",
      "",
      "# Configure pip to handle SSL issues",
      "export PIP_TRUSTED_HOST=\"pypi.org pypi.python.org files.pythonhosted.org\"",
      "export PIP_CERT=\"\"",
      "",
      "# Install Python packages if requirements.txt exists",
      "if [ -f requirements.txt ]; then",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org -q -r requirements.txt 2>/dev/null || echo 'Note: Some pip packages may have failed to install'",
      "elif [ ! -f venv/.packages_installed ]; then",
      "  echo 'Installing Python packages for image processing...'",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --upgrade pip 2>/dev/null || true",
      "  # Install PyTorch with CUDA 12.1 (compatible with system CUDA 12.0)",
      "  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 2>/dev/null || echo 'PyTorch installation may need manual intervention'",
      "  # Install vLLM and required packages",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org vllm transformers accelerate 'modelscope>=1.18.1' 2>/dev/null || echo 'Note: Some packages may not have installed'",
      "  # Install other utility packages",
      "  pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org pillow requests python-dotenv boto3 cairosvg 2>/dev/null || echo 'Note: Some packages may not have installed'",
      "  touch venv/.packages_installed",
      "fi",
      "",
      "# Install npm tools locally if not present",
      "if ! command -v wrangler >/dev/null 2>&1; then",
      "  echo 'Installing wrangler locally...'",
      "  npm install --prefix \"$HOME/.npm-global\" -g wrangler 2>/dev/null || echo 'wrangler may need to be installed separately'",
      "fi",
      "",
      "if ! command -v zuplo >/dev/null 2>&1; then",
      "  echo 'Installing zuplo CLI locally...'",
      "  npm install --prefix \"$HOME/.npm-global\" -g @zuplo/cli 2>/dev/null || echo 'zuplo CLI may need to be installed separately'",
      "fi",
      "",
      "# Install Playwright browsers and dependencies",
      "echo 'Installing Playwright browsers and dependencies...'",
      "npx playwright install-deps 2>/dev/null || echo 'Playwright dependencies installation may need manual intervention'",
      "npx playwright install 2>/dev/null || echo 'Playwright browsers installation may need manual intervention'",
      "",
      "# Auto-detect GPU and configure LLM backend",
      "echo 'Detecting hardware configuration for LLM inference...'",
      "# Find the project root - prefer current directory if devbox.json exists here",
      "if [ -f \"./devbox.json\" ]; then",
      "  PROJECT_ROOT=\".\"",
      "else",
      "  PROJECT_ROOT=\"$(dirname \"$(find . -maxdepth 3 -name devbox.json -type f 2>/dev/null | head -1)\" 2>/dev/null)\"",
      "  if [ -z \"$PROJECT_ROOT\" ]; then PROJECT_ROOT=\".\"; fi",
      "fi",
      "if [ -f \"$PROJECT_ROOT/backend/scripts/detect-gpu-config.sh\" ]; then",
      "  bash \"$PROJECT_ROOT/backend/scripts/detect-gpu-config.sh\" > /dev/null 2>&1",
      "  if [ -f \"$PROJECT_ROOT/backend/scripts/auto-config.json\" ]; then",
      "    # Parse the configuration",
      "    CONFIG_FILE=\"$PROJECT_ROOT/backend/scripts/auto-config.json\"",
      "    BACKEND=$(python3 -c \"import json; config=json.load(open('$CONFIG_FILE')); print(config.get('backend', 'unknown'))\" 2>/dev/null || echo 'unknown')",
      "    MODEL=$(python3 -c \"import json; config=json.load(open('$CONFIG_FILE')); print(config.get('model', 'unknown'))\" 2>/dev/null || echo 'unknown')",
      "    QUANTIZATION=$(python3 -c \"import json; config=json.load(open('$CONFIG_FILE')); print(config.get('quantization', 'None'))\" 2>/dev/null || echo 'None')",
      "    echo \"  ✓ Hardware detected: Backend=$BACKEND, Model=$(echo $MODEL | rev | cut -d'/' -f1 | rev)\"",
      "    if [ \"$QUANTIZATION\" != \"None\" ] && [ \"$QUANTIZATION\" != \"null\" ]; then",
      "      echo \"    Quantization: $QUANTIZATION\"",
      "    fi",
      "  else",
      "    echo '  ⚠ Hardware detection completed but no configuration generated'",
      "  fi",
      "else",
      "  echo '  ℹ GPU detection script not found. Run from project root to enable auto-configuration.'",
      "fi",
      "",
      "echo ''",
      "echo '🚀 iCraftStories Development Environment Ready'",
      "echo \"Node version: $(node -v)\"",
      "echo \"Python version: $(python3 --version)\"",
      "echo 'Virtual env: venv (activated)'",
      "echo 'NPM global packages: $HOME/.npm-global'",
      "",
      "# Show LLM server startup command if configured",
      "if [ -f \"$PROJECT_ROOT/backend/scripts/auto-config.json\" ]; then",
      "  echo ''",
      "  echo '💡 To start the LLM server with auto-detected configuration:'",
      "  echo '   python3 backend/scripts/auto-start-llm.py'",
      "  echo ''",
      "  echo '   Or use specific servers:'",
      "  if [ \"$BACKEND\" = \"vllm\" ] || [ \"$BACKEND\" = \"vllm-awq\" ]; then",
      "    echo '   python3 backend/scripts/run-vllm-server.py  # vLLM with Qwen2.5-VL'",
      "  elif [ \"$BACKEND\" = \"llama.cpp\" ]; then",
      "    echo '   ./llama-server -m models/qwen2.5-vl-7b-q4_k_m.gguf  # llama.cpp'",
      "  elif [ \"$BACKEND\" = \"mlx\" ]; then",
      "    echo '   python3 -m mlx_lm.server --model mlx-community/Qwen2.5-VL-7B-4bit  # MLX'",
      "  fi",
      "fi",
      "",
      "# Show image processing pipeline commands",
      "echo ''",
      "echo '🖼️  Image Processing Pipeline (D1-based):'",
      "echo '   cd backend/scripts'",
      "echo ''",
      "echo '   # 1️⃣  Initialize D1 Database (first time only):'",
      "echo '   ./init-d1-database.sh             # Create D1 database and categories'",
      "echo ''",
      "echo '   # 2️⃣  Sync Images from R2 to D1:'",
      "echo '   python3 sync-r2-to-d1.py           # Scan R2 and populate D1 database'",
      "echo ''",
      "echo '   # 3️⃣  Process Images with AI (includes thumbnail generation):'",
      "echo '   python3 process-images-vllm-s3.py  # Categorize with Qwen2.5-VL + thumbnails'",
      "echo ''",
      "echo '   # 4️⃣  Generate Embeddings for Search:'",
      "echo '   python3 generate-bge-m3-embeddings.py  # Create multilingual embeddings'",
      "echo ''",
      "echo '   # 5️⃣  Sync Everything to Supabase:'",
      "echo '   python3 sync-d1-to-supabase.py     # Deploy categories & images to production'",
      "echo ''",
      "echo '   # Optional: Convert local images to WebP and upload:'",
      "echo '   python3 convert-and-upload-to-r2.py /path/to/images'",
      "echo ''",
      "echo '📚 For more info: cat backend/scripts/README_IMAGE_PIPELINE.md'"
    ]
  },
  "env": {
    "NODE_ENV":    "development",
    "FORCE_COLOR": "1",
    "HF_HOME": "/home/g/hf-cache",
    "HUGGINGFACE_HUB_CACHE": "/home/g/hf-cache/hub",
    "VLLM_WORKER_MULTIPROC_METHOD": "spawn"
  }
}
